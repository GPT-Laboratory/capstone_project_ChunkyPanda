{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9ced48b01b344d0b8d78180f0e25354e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f8f5e02440245bc984c14e3caee9f75","IPY_MODEL_5178da08c4e5416b82e278275cdc218b","IPY_MODEL_c51f9672717f455887ef667581d89277"],"layout":"IPY_MODEL_a246cbf32cd44e22876666d67fe6b294"}},"2f8f5e02440245bc984c14e3caee9f75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce28649a8cb04345b4e7d1c8a0bbf2ce","placeholder":"​","style":"IPY_MODEL_d84f96b24ad547838c5eaeb9d3752d35","value":"spiece.model: 100%"}},"5178da08c4e5416b82e278275cdc218b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b54f46aaa874438a34f161ce78b64a2","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f39ef218dcc4092a6fdb934d1c5182f","value":791656}},"c51f9672717f455887ef667581d89277":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c95f6c8d00524958a4817f11118ba131","placeholder":"​","style":"IPY_MODEL_2700d38b26c64510a5339009b7e07d6b","value":" 792k/792k [00:00&lt;00:00, 4.23MB/s]"}},"a246cbf32cd44e22876666d67fe6b294":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce28649a8cb04345b4e7d1c8a0bbf2ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84f96b24ad547838c5eaeb9d3752d35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b54f46aaa874438a34f161ce78b64a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f39ef218dcc4092a6fdb934d1c5182f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c95f6c8d00524958a4817f11118ba131":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2700d38b26c64510a5339009b7e07d6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1da4a4de94974431855a2bed9733915e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02dea59c0d244d7e845a178178234590","IPY_MODEL_62991328a5d54a01be495e664ae6e471","IPY_MODEL_86b532e34294451fbe8dfcaa62aab9ad"],"layout":"IPY_MODEL_07062444b1dd449c8f948526c0ad01b2"}},"02dea59c0d244d7e845a178178234590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44b75ddd46a0496bbb9d251a44292192","placeholder":"​","style":"IPY_MODEL_592ccabb716345abb7069fdc462ffc25","value":"tokenizer.json: 100%"}},"62991328a5d54a01be495e664ae6e471":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da72d25b308a4d6eb4ec207a0ec1bb2c","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68be9565ef31457f935e1a8e3774f756","value":1389353}},"86b532e34294451fbe8dfcaa62aab9ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c092aea4949443faad9409a146e8aa6","placeholder":"​","style":"IPY_MODEL_b3acc8cd90794b65a68d1f7815c11ce8","value":" 1.39M/1.39M [00:00&lt;00:00, 5.47MB/s]"}},"07062444b1dd449c8f948526c0ad01b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b75ddd46a0496bbb9d251a44292192":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"592ccabb716345abb7069fdc462ffc25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da72d25b308a4d6eb4ec207a0ec1bb2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68be9565ef31457f935e1a8e3774f756":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c092aea4949443faad9409a146e8aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3acc8cd90794b65a68d1f7815c11ce8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c4ff13255324b7d8572bfd0562a06ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5edf725dc33c41b687efbd5be3335aac","IPY_MODEL_d4cc2aa6c844431d9bc58a32bb406bde","IPY_MODEL_42b43104163e4f90af8dc98d603bd95b"],"layout":"IPY_MODEL_b0cc878bb82e42c79466375e2aae9eac"}},"5edf725dc33c41b687efbd5be3335aac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddacdea8b196443ab6bad512eb700bf6","placeholder":"​","style":"IPY_MODEL_f917302cedf845b7b35440f8abf148e7","value":"config.json: 100%"}},"d4cc2aa6c844431d9bc58a32bb406bde":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea21768503847e69b142789b77b430c","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef905604f33149c0b62dcfe35dd70d7d","value":1208}},"42b43104163e4f90af8dc98d603bd95b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d1caef862424d388d6119d64bc33fca","placeholder":"​","style":"IPY_MODEL_5408d47569424705a3107b721e6be738","value":" 1.21k/1.21k [00:00&lt;00:00, 40.4kB/s]"}},"b0cc878bb82e42c79466375e2aae9eac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddacdea8b196443ab6bad512eb700bf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f917302cedf845b7b35440f8abf148e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ea21768503847e69b142789b77b430c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef905604f33149c0b62dcfe35dd70d7d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d1caef862424d388d6119d64bc33fca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5408d47569424705a3107b721e6be738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e17bc7cb57e646a7870c0857797fb0d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c39f741a003f4a71bb4bc79f8d816720","IPY_MODEL_205d632eb6d540e887e5ad4d3eadef6f","IPY_MODEL_6185cc50639a4702ac25f7c573a3faa0"],"layout":"IPY_MODEL_5ac70adc8cc241f3a17187720ed7519c"}},"c39f741a003f4a71bb4bc79f8d816720":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de0569b58553415baa5811e161ca4c0c","placeholder":"​","style":"IPY_MODEL_69d35b902a8e4a91820b6391460fa37b","value":"model.safetensors: 100%"}},"205d632eb6d540e887e5ad4d3eadef6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1d0fa73320547b0853cbca1b840f15b","max":891646390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15855f97fe87412b8bc94ef1a38e0e68","value":891646390}},"6185cc50639a4702ac25f7c573a3faa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_580983d78c9e40a6903b543ccf79c2e2","placeholder":"​","style":"IPY_MODEL_a8e5ca39dac84390b47884e19c45efd0","value":" 892M/892M [00:04&lt;00:00, 243MB/s]"}},"5ac70adc8cc241f3a17187720ed7519c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de0569b58553415baa5811e161ca4c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d35b902a8e4a91820b6391460fa37b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1d0fa73320547b0853cbca1b840f15b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15855f97fe87412b8bc94ef1a38e0e68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"580983d78c9e40a6903b543ccf79c2e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8e5ca39dac84390b47884e19c45efd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d794570b1c4c40ed87bcf5d3648ab3d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa203a1b0d3447db8430def25489d26f","IPY_MODEL_7189bfd535cc45f69e645e641788f950","IPY_MODEL_400ce653f8674e2c897c4e41035d2b77"],"layout":"IPY_MODEL_12388ae487354e2bbab87729837416e9"}},"aa203a1b0d3447db8430def25489d26f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76375ac5715f4f959524b4a961bfb2a9","placeholder":"​","style":"IPY_MODEL_f66f7a193f34421d85b3e5708c70c9f4","value":"generation_config.json: 100%"}},"7189bfd535cc45f69e645e641788f950":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_039426c2b3ee4a2d8e480341ca0723c4","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86d08c356b3242bd8ad9dee16628977c","value":147}},"400ce653f8674e2c897c4e41035d2b77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1caf12534d0a4b41ab901f78ed936ac2","placeholder":"​","style":"IPY_MODEL_0a0072beb1204c5a956101b97ad1e80c","value":" 147/147 [00:00&lt;00:00, 13.1kB/s]"}},"12388ae487354e2bbab87729837416e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76375ac5715f4f959524b4a961bfb2a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f66f7a193f34421d85b3e5708c70c9f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"039426c2b3ee4a2d8e480341ca0723c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d08c356b3242bd8ad9dee16628977c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1caf12534d0a4b41ab901f78ed936ac2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a0072beb1204c5a956101b97ad1e80c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c8fc95b1fcb4a16a588973c64b6abdf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_158b9714805d43c1a49f72050c6ae428","IPY_MODEL_5d11a261c9ec49d68e119081171fb543","IPY_MODEL_e701bad35e76412fa9f38e343574464b"],"layout":"IPY_MODEL_705a76ec0acc451b999ca560f77ce159"}},"158b9714805d43c1a49f72050c6ae428":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22c0fd8bf1264965a8671ca4a17af401","placeholder":"​","style":"IPY_MODEL_fe8758da5af94a87852136f284e324a6","value":"Epoch 1 Training: 100%"}},"5d11a261c9ec49d68e119081171fb543":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f953df5910a41dfa460785ad3e25d58","max":250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09df0f7de3fa481aacc6c4c0b7e09916","value":250}},"e701bad35e76412fa9f38e343574464b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95f8cf3831974ee5960336f5be47de8c","placeholder":"​","style":"IPY_MODEL_70d8ede85c4b4da1a7b602b9eada77d8","value":" 250/250 [01:14&lt;00:00,  4.05it/s, loss=20.3, hard_loss=2.3, soft_loss=7]"}},"705a76ec0acc451b999ca560f77ce159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22c0fd8bf1264965a8671ca4a17af401":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe8758da5af94a87852136f284e324a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f953df5910a41dfa460785ad3e25d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09df0f7de3fa481aacc6c4c0b7e09916":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95f8cf3831974ee5960336f5be47de8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70d8ede85c4b4da1a7b602b9eada77d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8b390434e8647afaba55076d4607302":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0231f13488e4fd1abf9a50ea04a6216","IPY_MODEL_1af3d91d54d142a2b95b6791c8ddd99d","IPY_MODEL_010b2be8138b4ae8b655a62e44882c12"],"layout":"IPY_MODEL_40ad335b5a4e42bdbfbe9535f2df05c1"}},"d0231f13488e4fd1abf9a50ea04a6216":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_449e3d5fdabd4aa48b0a69dee9264526","placeholder":"​","style":"IPY_MODEL_5fe3c095b4094832b473add3653b2458","value":"Evaluating: 100%"}},"1af3d91d54d142a2b95b6791c8ddd99d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d18a08d0e8034f62bcd336a421c1f3c0","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_024eba4c7aa344718534f5722643a83e","value":25}},"010b2be8138b4ae8b655a62e44882c12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f72e60ad2cc4eafbf917b2e530a5d97","placeholder":"​","style":"IPY_MODEL_daf0a26bfdc54ed3acf5d71ce704c161","value":" 25/25 [00:25&lt;00:00,  1.00it/s]"}},"40ad335b5a4e42bdbfbe9535f2df05c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"449e3d5fdabd4aa48b0a69dee9264526":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fe3c095b4094832b473add3653b2458":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d18a08d0e8034f62bcd336a421c1f3c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"024eba4c7aa344718534f5722643a83e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f72e60ad2cc4eafbf917b2e530a5d97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daf0a26bfdc54ed3acf5d71ce704c161":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c1e14d731c448de8b2c76d62f5b9bcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9caafb746a9740a2aff4aefe83d5e8e6","IPY_MODEL_bdccd2375dc045969f7286f0d1ccb315","IPY_MODEL_096e46fc3b324e6d9ef2e783875858b4"],"layout":"IPY_MODEL_d9dd5483105f42fe962e8182217a38f0"}},"9caafb746a9740a2aff4aefe83d5e8e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83caedeb8e334a51a7881f81f484a1b8","placeholder":"​","style":"IPY_MODEL_6444f98a0d624c79a7913f314117a146","value":"Epoch 2 Training: 100%"}},"bdccd2375dc045969f7286f0d1ccb315":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a936f2f551e4b9388f788bf16e69bc8","max":250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e114450cbe94f949aea6ae4ff42a2e2","value":250}},"096e46fc3b324e6d9ef2e783875858b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37577e1385c04f7d85b826b4586d4fa7","placeholder":"​","style":"IPY_MODEL_007f0fd72b4c41d9bbea9b8c0f8a4a25","value":" 250/250 [01:12&lt;00:00,  4.09it/s, loss=21.8, hard_loss=1.65, soft_loss=7.61]"}},"d9dd5483105f42fe962e8182217a38f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83caedeb8e334a51a7881f81f484a1b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6444f98a0d624c79a7913f314117a146":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a936f2f551e4b9388f788bf16e69bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e114450cbe94f949aea6ae4ff42a2e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37577e1385c04f7d85b826b4586d4fa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"007f0fd72b4c41d9bbea9b8c0f8a4a25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e12d6a50f35c42f3bef379af9e423398":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b0ef51075204e79b8b7b8fd0c5d4d5c","IPY_MODEL_0c470096acd644d380ebbb64411b58dc","IPY_MODEL_347cbf9c6dab4a3082c1dd86e0287ecf"],"layout":"IPY_MODEL_9267c77d58fa4f0b851b74f4f733f67c"}},"6b0ef51075204e79b8b7b8fd0c5d4d5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9aae2a059e84c089a0b1fbe8a310e77","placeholder":"​","style":"IPY_MODEL_d806e2a69a784d14be9b4f878c6ea208","value":"Evaluating: 100%"}},"0c470096acd644d380ebbb64411b58dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a35a444e02e4b58a21169bc26b04d00","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0cdabb45d574e7284e54b0b307f7e0f","value":25}},"347cbf9c6dab4a3082c1dd86e0287ecf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eb26d42d3214b6cb38fb07cdb7237f2","placeholder":"​","style":"IPY_MODEL_d96ac70b0d2640cdb4dea58dc55b2a30","value":" 25/25 [00:26&lt;00:00,  1.08s/it]"}},"9267c77d58fa4f0b851b74f4f733f67c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9aae2a059e84c089a0b1fbe8a310e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d806e2a69a784d14be9b4f878c6ea208":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a35a444e02e4b58a21169bc26b04d00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0cdabb45d574e7284e54b0b307f7e0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7eb26d42d3214b6cb38fb07cdb7237f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d96ac70b0d2640cdb4dea58dc55b2a30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97cdaa6cc0fc4de5b06ef5ac89ef4185":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fd74883dcf14399a85f2760486ea711","IPY_MODEL_d9b4975d3aac470798f415b9b403c2a7","IPY_MODEL_6c2f5ddfc51542b4930e0be4e341d61c"],"layout":"IPY_MODEL_95b563491b0c4c769a537d5248aaaa0d"}},"0fd74883dcf14399a85f2760486ea711":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1e68ff32dc948168fb377859abd60ae","placeholder":"​","style":"IPY_MODEL_77b7e8c4fa1746d0869da0bca8d12b81","value":"Epoch 3 Training: 100%"}},"d9b4975d3aac470798f415b9b403c2a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_596c5c3b113549db879feac4541385eb","max":250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79cbd014396a4dff97d94f3016241e45","value":250}},"6c2f5ddfc51542b4930e0be4e341d61c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2db1d31f51184d5e97e5d44f67aa5a9a","placeholder":"​","style":"IPY_MODEL_645debeca8a74b339c7f332b6724892f","value":" 250/250 [01:12&lt;00:00,  3.93it/s, loss=22.4, hard_loss=2.16, soft_loss=7.75]"}},"95b563491b0c4c769a537d5248aaaa0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1e68ff32dc948168fb377859abd60ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77b7e8c4fa1746d0869da0bca8d12b81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"596c5c3b113549db879feac4541385eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79cbd014396a4dff97d94f3016241e45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2db1d31f51184d5e97e5d44f67aa5a9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"645debeca8a74b339c7f332b6724892f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"239b34b82dc344c1ae435855ae9f6c49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dbbcc88bad6a403aa30038332bbfb21d","IPY_MODEL_5316a68461eb4650a916862471c716ec","IPY_MODEL_47f46578bf0f4cbda4e4fd3553b6a16e"],"layout":"IPY_MODEL_fafde6b6c05c4010badbd86799e4625f"}},"dbbcc88bad6a403aa30038332bbfb21d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29d91d8d36a44fe0a313516e566c2c1d","placeholder":"​","style":"IPY_MODEL_aaf8d0efb93348f69dcb7427e4f1f96f","value":"Evaluating: 100%"}},"5316a68461eb4650a916862471c716ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82191c64038340b5bedc19007537c64b","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0b83465a1f54f7a8aa25e238b139533","value":25}},"47f46578bf0f4cbda4e4fd3553b6a16e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f70322776dc942e08af3a29a91614b4e","placeholder":"​","style":"IPY_MODEL_6b0ab33fc5e943389fa52bcc2d2b2e0f","value":" 25/25 [00:28&lt;00:00,  1.14s/it]"}},"fafde6b6c05c4010badbd86799e4625f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29d91d8d36a44fe0a313516e566c2c1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaf8d0efb93348f69dcb7427e4f1f96f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82191c64038340b5bedc19007537c64b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0b83465a1f54f7a8aa25e238b139533":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f70322776dc942e08af3a29a91614b4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b0ab33fc5e943389fa52bcc2d2b2e0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Distillation for Efficient AI Deployment","metadata":{}},{"cell_type":"markdown","source":"## Install libraries","metadata":{}},{"cell_type":"code","source":"!pip install datasets torch transformers rouge","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LW2wogAj06qD","outputId":"83dad0ee-ec69-4a8e-ee03-26de3e34e941","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:25:33.664421Z","iopub.execute_input":"2025-04-02T09:25:33.664598Z","iopub.status.idle":"2025-04-02T09:25:40.577798Z","shell.execute_reply.started":"2025-04-02T09:25:33.664580Z","shell.execute_reply":"2025-04-02T09:25:40.576775Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import required libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_scheduler\n)\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset\nfrom tqdm.notebook import tqdm\nimport wandb\nimport gc","metadata":{"id":"SpWFIaqr0Il9","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:27:01.485719Z","iopub.execute_input":"2025-04-02T09:27:01.486005Z","iopub.status.idle":"2025-04-02T09:27:36.805217Z","shell.execute_reply.started":"2025-04-02T09:27:01.485980Z","shell.execute_reply":"2025-04-02T09:27:36.804580Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Check GPU availability\nif torch.cuda.is_available():\n    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n    print(f\"Available GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**2:.2f} MB\")\n    print(f\"Current GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n    !nvidia-smi\n\n# Set seed for reproducibility\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n\nset_seed()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mA_O4_Y3FLe","outputId":"c9f1dfcb-5eed-4f10-bccb-8cc38f4b0df1","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:27:51.546206Z","iopub.execute_input":"2025-04-02T09:27:51.546489Z","iopub.status.idle":"2025-04-02T09:27:51.919795Z","shell.execute_reply.started":"2025-04-02T09:27:51.546465Z","shell.execute_reply":"2025-04-02T09:27:51.918975Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU Model: Tesla T4\nAvailable GPU Memory: 15095.06 MB\nCurrent GPU Memory Usage: 0.00 MB\nWed Apr  2 09:27:51 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhugging_face_token = user_secrets.get_secret(\"Hugging_Face_Token\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Distillation-T5', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:27:58.305616Z","iopub.execute_input":"2025-04-02T09:27:58.305937Z","iopub.status.idle":"2025-04-02T09:28:10.705772Z","shell.execute_reply.started":"2025-04-02T09:27:58.305912Z","shell.execute_reply":"2025-04-02T09:28:10.704906Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwenxupine\u001b[0m (\u001b[33mwenxupine-tampere-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250402_092804-epodi2na</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/wenxupine-tampere-university/Distillation-T5/runs/epodi2na' target=\"_blank\">colorful-dawn-3</a></strong> to <a href='https://wandb.ai/wenxupine-tampere-university/Distillation-T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/wenxupine-tampere-university/Distillation-T5' target=\"_blank\">https://wandb.ai/wenxupine-tampere-university/Distillation-T5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/wenxupine-tampere-university/Distillation-T5/runs/epodi2na' target=\"_blank\">https://wandb.ai/wenxupine-tampere-university/Distillation-T5/runs/epodi2na</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Define Summarization Dataset class\nclass SummarizationDataset(Dataset):\n    def __init__(self, texts, summaries, tokenizer, max_source_length=512, max_target_length=128):\n        self.texts = texts\n        self.summaries = summaries\n        self.tokenizer = tokenizer\n        self.max_source_length = max_source_length\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        summary = self.summaries[idx]\n\n        # Add task prefix to input\n        source_text = f\"summarize: {text}\"\n\n        # Tokenize input text\n        source_encoding = self.tokenizer(\n            source_text,\n            max_length=self.max_source_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        # Tokenize target summary\n        target_encoding = self.tokenizer(\n            summary,\n            max_length=self.max_target_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = source_encoding[\"input_ids\"].squeeze()\n        attention_mask = source_encoding[\"attention_mask\"].squeeze()\n        labels = target_encoding[\"input_ids\"].squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100  # Ignore pad tokens in loss\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"text\": text,\n            \"summary\": summary\n        }\n\n# Load dataset function\ndef load_dataset_for_distillation(tokenizer, batch_size=4):\n    # Using CNN/DailyMail dataset as an example\n    print(\"Loading dataset...\")\n    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n\n    # Extract training and validation sets\n    # Limiting to smaller subsets for faster training\n    train_texts = dataset[\"train\"][\"article\"][:1000]\n    train_summaries = dataset[\"train\"][\"highlights\"][:1000]\n\n    val_texts = dataset[\"validation\"][\"article\"][:200]\n    val_summaries = dataset[\"validation\"][\"highlights\"][:200]\n\n    # Create datasets\n    train_dataset = SummarizationDataset(train_texts, train_summaries, tokenizer)\n    val_dataset = SummarizationDataset(val_texts, val_summaries, tokenizer)\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, val_loader","metadata":{"id":"mtwTy4at3Nn2","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:41:51.366217Z","iopub.execute_input":"2025-04-02T09:41:51.366508Z","iopub.status.idle":"2025-04-02T09:41:51.374867Z","shell.execute_reply.started":"2025-04-02T09:41:51.366486Z","shell.execute_reply":"2025-04-02T09:41:51.374011Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Load models and tokenizers\ndef load_models_and_tokenizers():\n    # Using T5-base as teacher model (much smaller than T5-3B)\n    print(\"Loading teacher model (T5-base)...\")\n    teacher_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    teacher_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n    teacher_model.eval()  # Set to evaluation mode\n\n    print(\"Loading student model (T5-small)...\")\n    student_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n    student_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n\n    return teacher_model, teacher_tokenizer, student_model, student_tokenizer\n\n# Memory optimization function\ndef optimize_memory():\n    # Clear cache\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # Print memory usage\n    if torch.cuda.is_available():\n        print(f\"Current GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n\n# Load models\nteacher_model, teacher_tokenizer, student_model, student_tokenizer = load_models_and_tokenizers()\noptimize_memory()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246,"referenced_widgets":["9ced48b01b344d0b8d78180f0e25354e","2f8f5e02440245bc984c14e3caee9f75","5178da08c4e5416b82e278275cdc218b","c51f9672717f455887ef667581d89277","a246cbf32cd44e22876666d67fe6b294","ce28649a8cb04345b4e7d1c8a0bbf2ce","d84f96b24ad547838c5eaeb9d3752d35","5b54f46aaa874438a34f161ce78b64a2","3f39ef218dcc4092a6fdb934d1c5182f","c95f6c8d00524958a4817f11118ba131","2700d38b26c64510a5339009b7e07d6b","1da4a4de94974431855a2bed9733915e","02dea59c0d244d7e845a178178234590","62991328a5d54a01be495e664ae6e471","86b532e34294451fbe8dfcaa62aab9ad","07062444b1dd449c8f948526c0ad01b2","44b75ddd46a0496bbb9d251a44292192","592ccabb716345abb7069fdc462ffc25","da72d25b308a4d6eb4ec207a0ec1bb2c","68be9565ef31457f935e1a8e3774f756","8c092aea4949443faad9409a146e8aa6","b3acc8cd90794b65a68d1f7815c11ce8","5c4ff13255324b7d8572bfd0562a06ec","5edf725dc33c41b687efbd5be3335aac","d4cc2aa6c844431d9bc58a32bb406bde","42b43104163e4f90af8dc98d603bd95b","b0cc878bb82e42c79466375e2aae9eac","ddacdea8b196443ab6bad512eb700bf6","f917302cedf845b7b35440f8abf148e7","8ea21768503847e69b142789b77b430c","ef905604f33149c0b62dcfe35dd70d7d","8d1caef862424d388d6119d64bc33fca","5408d47569424705a3107b721e6be738","e17bc7cb57e646a7870c0857797fb0d1","c39f741a003f4a71bb4bc79f8d816720","205d632eb6d540e887e5ad4d3eadef6f","6185cc50639a4702ac25f7c573a3faa0","5ac70adc8cc241f3a17187720ed7519c","de0569b58553415baa5811e161ca4c0c","69d35b902a8e4a91820b6391460fa37b","f1d0fa73320547b0853cbca1b840f15b","15855f97fe87412b8bc94ef1a38e0e68","580983d78c9e40a6903b543ccf79c2e2","a8e5ca39dac84390b47884e19c45efd0","d794570b1c4c40ed87bcf5d3648ab3d9","aa203a1b0d3447db8430def25489d26f","7189bfd535cc45f69e645e641788f950","400ce653f8674e2c897c4e41035d2b77","12388ae487354e2bbab87729837416e9","76375ac5715f4f959524b4a961bfb2a9","f66f7a193f34421d85b3e5708c70c9f4","039426c2b3ee4a2d8e480341ca0723c4","86d08c356b3242bd8ad9dee16628977c","1caf12534d0a4b41ab901f78ed936ac2","0a0072beb1204c5a956101b97ad1e80c"]},"id":"uj-4DNGW3ST-","outputId":"061206ce-e3f5-44c4-9b0f-2e5c91adbf22","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:41:57.756065Z","iopub.execute_input":"2025-04-02T09:41:57.756336Z","iopub.status.idle":"2025-04-02T09:41:59.978628Z","shell.execute_reply.started":"2025-04-02T09:41:57.756315Z","shell.execute_reply":"2025-04-02T09:41:59.977910Z"}},"outputs":[{"name":"stdout","text":"Loading teacher model (T5-base)...\nLoading student model (T5-small)...\nCurrent GPU Memory Usage: 3083.84 MB\nGPU Memory Cached: 3356.00 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Define Distillation Loss\nclass DistillationLoss(nn.Module):\n    def __init__(self, temperature=2.0, alpha=0.5):\n        super(DistillationLoss, self).__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n\n    def forward(self, student_outputs, teacher_outputs):\n        # Get logits\n        student_logits = student_outputs.logits\n        teacher_logits = teacher_outputs.logits\n\n        # Get teacher's distribution\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)\n\n        # Calculate student's soft target loss\n        soft_targets_loss = -(teacher_probs * F.log_softmax(student_logits / self.temperature, dim=-1)).sum(dim=-1).mean()\n\n        # Use student_outputs' loss as hard target loss\n        hard_targets_loss = student_outputs.loss\n\n        # Combine soft and hard target losses\n        loss = self.alpha * (self.temperature ** 2) * soft_targets_loss + (1 - self.alpha) * hard_targets_loss\n\n        return loss, hard_targets_loss, soft_targets_loss\n\n# Initialize distillation loss function\ndistillation_loss_fn = DistillationLoss(temperature=2.0, alpha=0.7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:42:06.544725Z","iopub.execute_input":"2025-04-02T09:42:06.545015Z","iopub.status.idle":"2025-04-02T09:42:06.551475Z","shell.execute_reply.started":"2025-04-02T09:42:06.544991Z","shell.execute_reply":"2025-04-02T09:42:06.550645Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Training step function\n# Fixed train step function - Run this first before starting training\ndef train_step(teacher_model, student_model, train_loader, optimizer, scheduler, distillation_loss_fn, epoch):\n    student_model.train()\n    total_loss = 0\n    total_hard_loss = 0\n    total_soft_loss = 0\n\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")\n\n    for batch_idx, batch in enumerate(progress_bar):  # Added batch_idx here\n        # Move data to device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Clear gradients\n        optimizer.zero_grad()\n\n        # Forward pass - Student model\n        student_outputs = student_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        # Get Teacher model outputs (no gradient computation)\n        with torch.no_grad():\n            teacher_outputs = teacher_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n        # Calculate distillation loss\n        loss, hard_loss, soft_loss = distillation_loss_fn(student_outputs, teacher_outputs)\n\n        # Backward pass\n        loss.backward()\n\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n\n        # Update parameters\n        optimizer.step()\n        scheduler.step()\n\n        # Update loss\n        total_loss += loss.item()\n        total_hard_loss += hard_loss.item()\n        total_soft_loss += soft_loss.item()\n\n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': loss.item(),\n            'hard_loss': hard_loss.item(),\n            'soft_loss': soft_loss.item()\n        })\n\n        # Free memory\n        del input_ids, attention_mask, labels, student_outputs, teacher_outputs, loss\n        if batch_idx % 10 == 0:  # Every 10 batches\n            optimize_memory()\n\n    avg_loss = total_loss / len(train_loader)\n    avg_hard_loss = total_hard_loss / len(train_loader)\n    avg_soft_loss = total_soft_loss / len(train_loader)\n\n    return avg_loss, avg_hard_loss, avg_soft_loss\n\n# Evaluation function\ndef evaluate(student_model, val_loader):\n    student_model.eval()\n    generated_summaries = []\n    reference_summaries = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            # Generate summaries\n            summary_ids = student_model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True\n            )\n\n            # Convert generated IDs to text\n            decoded_summaries = [student_tokenizer.decode(g, skip_special_tokens=True) for g in summary_ids]\n            generated_summaries.extend(decoded_summaries)\n            reference_summaries.extend(batch[\"summary\"])\n\n            # Free memory\n            del input_ids, attention_mask, summary_ids\n\n    # Return generated summaries and references\n    return generated_summaries, reference_summaries","metadata":{"id":"HLLGjD8z3jn2","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:42:13.478005Z","iopub.execute_input":"2025-04-02T09:42:13.478314Z","iopub.status.idle":"2025-04-02T09:42:13.487946Z","shell.execute_reply.started":"2025-04-02T09:42:13.478287Z","shell.execute_reply":"2025-04-02T09:42:13.486913Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Load dataset\nbatch_size = 2  # Small batch size to save memory\ntrain_loader, val_loader = load_dataset_for_distillation(student_tokenizer, batch_size=batch_size)\n# Calculate total training steps\nnum_epochs = 3\ntotal_steps = len(train_loader) * num_epochs\n# Better optimizer configuration\noptimizer = AdamW(student_model.parameters(), \n                 lr=3e-5,  # learning rate\n                 weight_decay=0.01)\n\n# Use cosine scheduler instead of linear\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=100,\n    num_training_steps=total_steps\n)\nprint(f\"Total training steps: {total_steps}\")\nprint(f\"Training with batch size: {batch_size}\")\nprint(f\"Number of training examples: {len(train_loader.dataset)}\")\nprint(f\"Number of validation examples: {len(val_loader.dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:42:33.811021Z","iopub.execute_input":"2025-04-02T09:42:33.811310Z","iopub.status.idle":"2025-04-02T09:42:37.063432Z","shell.execute_reply.started":"2025-04-02T09:42:33.811288Z","shell.execute_reply":"2025-04-02T09:42:37.062731Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nTotal training steps: 1500\nTraining with batch size: 2\nNumber of training examples: 1000\nNumber of validation examples: 200\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"Starting distillation training...\")\n\nfor epoch in range(num_epochs):\n    # Train one epoch\n    avg_loss, avg_hard_loss, avg_soft_loss = train_step(\n        teacher_model, student_model, train_loader, optimizer, scheduler, distillation_loss_fn, epoch\n    )\n\n    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}, Hard Target Loss: {avg_hard_loss:.4f}, Soft Target Loss: {avg_soft_loss:.4f}\")\n\n    # Generate and evaluate summaries every epoch\n    print(\"Generating summary examples...\")\n    generated_summaries, reference_summaries = evaluate(student_model, val_loader)\n\n    # Print some examples\n    for i in range(min(3, len(generated_summaries))):\n        print(f\"\\nReference Summary: {reference_summaries[i]}\")\n        print(f\"Generated Summary: {generated_summaries[i]}\")\n        print(\"-\" * 50)\n\n    # Save model checkpoint\n    checkpoint_path = f\"t5_small_distilled_epoch_{epoch+1}\"\n    student_model.save_pretrained(checkpoint_path)\n    student_tokenizer.save_pretrained(checkpoint_path)\n    print(f\"Saved model checkpoint: {checkpoint_path}\")\n\n    # Free memory\n    optimize_memory()\n\nprint(\"Distillation training completed!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8c8fc95b1fcb4a16a588973c64b6abdf","158b9714805d43c1a49f72050c6ae428","5d11a261c9ec49d68e119081171fb543","e701bad35e76412fa9f38e343574464b","705a76ec0acc451b999ca560f77ce159","22c0fd8bf1264965a8671ca4a17af401","fe8758da5af94a87852136f284e324a6","2f953df5910a41dfa460785ad3e25d58","09df0f7de3fa481aacc6c4c0b7e09916","95f8cf3831974ee5960336f5be47de8c","70d8ede85c4b4da1a7b602b9eada77d8","e8b390434e8647afaba55076d4607302","d0231f13488e4fd1abf9a50ea04a6216","1af3d91d54d142a2b95b6791c8ddd99d","010b2be8138b4ae8b655a62e44882c12","40ad335b5a4e42bdbfbe9535f2df05c1","449e3d5fdabd4aa48b0a69dee9264526","5fe3c095b4094832b473add3653b2458","d18a08d0e8034f62bcd336a421c1f3c0","024eba4c7aa344718534f5722643a83e","9f72e60ad2cc4eafbf917b2e530a5d97","daf0a26bfdc54ed3acf5d71ce704c161","8c1e14d731c448de8b2c76d62f5b9bcf","9caafb746a9740a2aff4aefe83d5e8e6","bdccd2375dc045969f7286f0d1ccb315","096e46fc3b324e6d9ef2e783875858b4","d9dd5483105f42fe962e8182217a38f0","83caedeb8e334a51a7881f81f484a1b8","6444f98a0d624c79a7913f314117a146","4a936f2f551e4b9388f788bf16e69bc8","8e114450cbe94f949aea6ae4ff42a2e2","37577e1385c04f7d85b826b4586d4fa7","007f0fd72b4c41d9bbea9b8c0f8a4a25","e12d6a50f35c42f3bef379af9e423398","6b0ef51075204e79b8b7b8fd0c5d4d5c","0c470096acd644d380ebbb64411b58dc","347cbf9c6dab4a3082c1dd86e0287ecf","9267c77d58fa4f0b851b74f4f733f67c","c9aae2a059e84c089a0b1fbe8a310e77","d806e2a69a784d14be9b4f878c6ea208","6a35a444e02e4b58a21169bc26b04d00","d0cdabb45d574e7284e54b0b307f7e0f","7eb26d42d3214b6cb38fb07cdb7237f2","d96ac70b0d2640cdb4dea58dc55b2a30","97cdaa6cc0fc4de5b06ef5ac89ef4185","0fd74883dcf14399a85f2760486ea711","d9b4975d3aac470798f415b9b403c2a7","6c2f5ddfc51542b4930e0be4e341d61c","95b563491b0c4c769a537d5248aaaa0d","a1e68ff32dc948168fb377859abd60ae","77b7e8c4fa1746d0869da0bca8d12b81","596c5c3b113549db879feac4541385eb","79cbd014396a4dff97d94f3016241e45","2db1d31f51184d5e97e5d44f67aa5a9a","645debeca8a74b339c7f332b6724892f","239b34b82dc344c1ae435855ae9f6c49","dbbcc88bad6a403aa30038332bbfb21d","5316a68461eb4650a916862471c716ec","47f46578bf0f4cbda4e4fd3553b6a16e","fafde6b6c05c4010badbd86799e4625f","29d91d8d36a44fe0a313516e566c2c1d","aaf8d0efb93348f69dcb7427e4f1f96f","82191c64038340b5bedc19007537c64b","b0b83465a1f54f7a8aa25e238b139533","f70322776dc942e08af3a29a91614b4e","6b0ab33fc5e943389fa52bcc2d2b2e0f"]},"id":"HOV7E3jo3qRO","outputId":"7afc44db-deb6-4609-fe83-8e58f9ceb77e","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:42:47.222318Z","iopub.execute_input":"2025-04-02T09:42:47.222624Z","iopub.status.idle":"2025-04-02T09:54:51.153143Z","shell.execute_reply.started":"2025-04-02T09:42:47.222596Z","shell.execute_reply":"2025-04-02T09:54:51.152288Z"}},"outputs":[{"name":"stdout","text":"Starting distillation training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1 Training:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73809ed6cdcb4c86987ecaf4fd972e09"}},"metadata":{}},{"name":"stdout","text":"Current GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4090.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4216.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4192.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4212.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4272.00 MB\nCurrent GPU Memory Usage: 3785.41 MB\nGPU Memory Cached: 4228.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4268.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nEpoch 1 - Average Loss: 21.8720, Hard Target Loss: 2.4879, Soft Target Loss: 7.5449\nGenerating summary examples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac38f8ba30041798046a502a654d7a2"}},"metadata":{}},{"name":"stdout","text":"\nReference Summary: Zully Broussard decided to give a kidney to a stranger .\nA new computer program helped her donation spur transplants for six kidney patients .\nGenerated Summary: \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a life extension, that's pretty big,\" a comment on a Facebook page read. five surgeons, a covey of physician assistants, nurses, anesthesiologists, and more than 40 support staff perform surgeries on 12 people. the chain of surgeries is to be wrapped up Friday.\n--------------------------------------------------\n\nReference Summary: The 20th MLS season begins this weekend .\nLeague has changed dramatically since its inception in 1996 .\nSome question whether rules regarding salary caps and transfers need to change .\nGenerated Summary: MLS is the first of a new domestic television and media rights deal with FOX, ESPN and Univision. the new season is the first of a new domestic television and media rights deal with FOX, ESPN and Univision. a further four are set to be added by 2020.\n--------------------------------------------------\n\nReference Summary: Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\nBut he reportedly left the pitch conscious and wearing an oxygen mask .\nGomis later said that he was \"feeling well\"\nThe incident came three years after Fabrice Muamba collapsed at White Hart Lane .\nGenerated Summary: french striker Bafetimbi Gomis is \"feeling well\" after collapsing during Swansea's 3-2 defeat at Tottenham. the 29-year-old left the pitch conscious after about five minutes of treatment. Gomis spent the night in hospital as a precaution, according to Swansea.\n--------------------------------------------------\nSaved model checkpoint: t5_small_distilled_epoch_1\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2 Training:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d11f707e0df4645be82ed5d0601620a"}},"metadata":{}},{"name":"stdout","text":"Current GPU Memory Usage: 3785.41 MB\nGPU Memory Cached: 4272.00 MB\nCurrent GPU Memory Usage: 3785.41 MB\nGPU Memory Cached: 4228.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nEpoch 2 - Average Loss: 21.7609, Hard Target Loss: 2.3733, Soft Target Loss: 7.5175\nGenerating summary examples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122aa774c23a4625964aaa793003c7cd"}},"metadata":{}},{"name":"stdout","text":"\nReference Summary: Zully Broussard decided to give a kidney to a stranger .\nA new computer program helped her donation spur transplants for six kidney patients .\nGenerated Summary: \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a life extension, that's pretty big,\" a comment on a Facebook page read. \"I know this entire journey is much bigger than all of us,\" a comment on a Facebook page read. five surgeons, a covey of physician assistants, nurses, anesthesiologists, and more than 40 support staff perform surgeries.\n--------------------------------------------------\n\nReference Summary: The 20th MLS season begins this weekend .\nLeague has changed dramatically since its inception in 1996 .\nSome question whether rules regarding salary caps and transfers need to change .\nGenerated Summary: MLS is the first of a new domestic television and media rights deal with FOX, ESPN and Univision. the new season is the first of a new domestic television and media rights deal with FOX, ESPN and Univision. MLS has a record of 20 teams in the past eight years.\n--------------------------------------------------\n\nReference Summary: Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\nBut he reportedly left the pitch conscious and wearing an oxygen mask .\nGomis later said that he was \"feeling well\"\nThe incident came three years after Fabrice Muamba collapsed at White Hart Lane .\nGenerated Summary: french striker Bafetimbi Gomis is \"feeling well\" after collapsing during Swansea's 3-2 defeat at Tottenham. the 29-year-old left the pitch conscious after about five minutes of treatment. Gomis spent the night in hospital as a precaution, Swansea said.\n--------------------------------------------------\nSaved model checkpoint: t5_small_distilled_epoch_2\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3 Training:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f956030d824469b8d1f65bf889a37c"}},"metadata":{}},{"name":"stdout","text":"Current GPU Memory Usage: 3785.41 MB\nGPU Memory Cached: 4272.00 MB\nCurrent GPU Memory Usage: 3785.41 MB\nGPU Memory Cached: 4228.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nEpoch 3 - Average Loss: 21.7338, Hard Target Loss: 2.3407, Soft Target Loss: 7.5113\nGenerating summary examples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f0b63e5b3e4ee9aa2976c20f00cedd"}},"metadata":{}},{"name":"stdout","text":"\nReference Summary: Zully Broussard decided to give a kidney to a stranger .\nA new computer program helped her donation spur transplants for six kidney patients .\nGenerated Summary: \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a life extension, that's pretty big,\" she says. \"I know this entire journey is much bigger than all of us. I also know I'm just the messenger,\" she says. \"the ages of the donors and recipients range from 26 to 70,\" the medical center says.\n--------------------------------------------------\n\nReference Summary: The 20th MLS season begins this weekend .\nLeague has changed dramatically since its inception in 1996 .\nSome question whether rules regarding salary caps and transfers need to change .\nGenerated Summary: MLS is the first of a new domestic television and media rights deal with FOX, ESPN and Univision. the new season is the first of a new domestic television and media rights deal with FOX, ESPN and Univision. MLS has a record of 50 years of growth in most other industries.\n--------------------------------------------------\n\nReference Summary: Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\nBut he reportedly left the pitch conscious and wearing an oxygen mask .\nGomis later said that he was \"feeling well\"\nThe incident came three years after Fabrice Muamba collapsed at White Hart Lane .\nGenerated Summary: french striker Bafetimbi Gomis is \"feeling well\" after collapsing during Swansea's 3-2 defeat at Tottenham. Gomis has a history of fainting and has a history of fainting. the 29-year-old left the pitch conscious after about five minutes of treatment.\n--------------------------------------------------\nSaved model checkpoint: t5_small_distilled_epoch_3\nCurrent GPU Memory Usage: 3784.41 MB\nGPU Memory Cached: 4248.00 MB\nDistillation training completed!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"repo_id = \"Wenfi/distillation-T5-cnn\"\n\n# Save model and tokenizer locally\nsave_path = \"./student_model_distilled\"\nstudent_model.save_pretrained(save_path)\nstudent_tokenizer.save_pretrained(save_path)\n\n# Push to Hugging Face\nstudent_model.push_to_hub(repo_id)\nstudent_tokenizer.push_to_hub(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:55:59.407685Z","iopub.execute_input":"2025-04-02T09:55:59.408028Z","iopub.status.idle":"2025-04-02T09:56:08.061461Z","shell.execute_reply.started":"2025-04-02T09:55:59.407999Z","shell.execute_reply":"2025-04-02T09:56:08.060836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d93f411fc284383ab2dcdfa8f119596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ae5074474ad4a5c93e58414e85a0393"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Wenfi/distillation-T5-cnn/commit/8e71fdae0b63748927ff3c80eb0c1b5ed11cb068', commit_message='Upload tokenizer', commit_description='', oid='8e71fdae0b63748927ff3c80eb0c1b5ed11cb068', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Wenfi/distillation-T5-cnn', endpoint='https://huggingface.co', repo_type='model', repo_id='Wenfi/distillation-T5-cnn'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"# Inference function for the distilled model\ndef generate_summary(model, tokenizer, text, max_length=150):\n    # Prepare input\n    input_text = f\"summarize: {text}\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # Generate summary\n    summary_ids = model.generate(\n        input_ids,\n        max_length=max_length,\n        num_beams=4,\n        early_stopping=True,\n        no_repeat_ngram_size=2\n    )\n\n    # Decode summary\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Compare original and distilled model's output for a given text\ndef compare_models(text):\n    # Generate with teacher model\n    teacher_summary = generate_summary(teacher_model, teacher_tokenizer, text)\n\n    # Generate with student model\n    student_summary = generate_summary(student_model, student_tokenizer, text)\n\n    print(\"Original Text:\")\n    print(text[:500] + \"...\" if len(text) > 500 else text)\n    print(\"\\nTeacher Model Summary:\")\n    print(teacher_summary)\n    print(\"\\nDistilled Student Model Summary:\")\n    print(student_summary)\n\n# Example usage\nsample_text = \"\"\"\nClimate change is the long-term alteration of temperature and typical weather patterns in a place.\nClimate change could refer to a particular location or the planet as a whole. Climate change may\ncause weather patterns to be less predictable. These unexpected weather patterns can make it\ndifficult to maintain and grow crops in regions that rely on farming because expected temperature\nand rainfall levels can no longer be relied on. Climate change has also been connected with other\ndamaging weather events such as more frequent and more intense hurricanes, floods, downpours, and\nwinter storms. In polar regions, the warming global temperatures associated with climate change have\nmeant ice sheets and glaciers are melting at an accelerated rate from season to season. This contributes\nto sea levels rising in different regions of the planet. Together with expanding ocean waters due to\nrising temperatures, the resulting rise in sea level has begun to damage coastlines as a result of\nincreased flooding and erosion.\n\"\"\"\n\ncompare_models(sample_text)","metadata":{"id":"miFacGlF4Dy6","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:56:16.317492Z","iopub.execute_input":"2025-04-02T09:56:16.317806Z","iopub.status.idle":"2025-04-02T09:56:19.116761Z","shell.execute_reply.started":"2025-04-02T09:56:16.317778Z","shell.execute_reply":"2025-04-02T09:56:19.115886Z"}},"outputs":[{"name":"stdout","text":"Original Text:\n\nClimate change is the long-term alteration of temperature and typical weather patterns in a place.\nClimate change could refer to a particular location or the planet as a whole. Climate change may\ncause weather patterns to be less predictable. These unexpected weather patterns can make it\ndifficult to maintain and grow crops in regions that rely on farming because expected temperature\nand rainfall levels can no longer be relied on. Climate change has also been connected with other\ndamaging weath...\n\nTeacher Model Summary:\nclimate change is the long-term alteration of temperature and typical weather patterns in a place . this can make it difficult to maintain and grow crops in regions that rely on farming if temperatures and rainfall levels can no longer be relied on.\n\nDistilled Student Model Summary:\nclimate change is the long-term alteration of temperature and typical weather patterns in a place. polar regions have been linked to more frequent and more intense hurricanes, flooding, downpours and winter storms. this contributes to sea levels rising in different regions of the planet.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Function to compare original and distilled models's summaries with teacher models' summaries\ndef compare_original_vs_distilled(text, original_model_path=\"t5-small\"):\n    \"\"\"\n    Compare summaries generated by the original student model and the distilled model.\n\n    Args:\n        text (str): Input text to summarize\n        original_model_path (str): Path to the original model (default: \"t5-small\")\n\n    Returns:\n        None: Prints the comparison results\n    \"\"\"\n    # Load the original student model\n    print(\"Loading original student model...\")\n    original_model = T5ForConditionalGeneration.from_pretrained(original_model_path).to(device)\n    original_tokenizer = T5Tokenizer.from_pretrained(original_model_path)\n\n    # Load the latest distilled model checkpoint (assuming it exists)\n    distilled_model_path = f\"t5_small_distilled_epoch_{num_epochs}\"\n\n    try:\n        print(f\"Loading distilled model from {distilled_model_path}...\")\n        distilled_model = T5ForConditionalGeneration.from_pretrained(distilled_model_path).to(device)\n        distilled_tokenizer = T5Tokenizer.from_pretrained(distilled_model_path)\n    except:\n        print(\"Couldn't find the distilled model checkpoint. Using current student model...\")\n        distilled_model = student_model\n        distilled_tokenizer = student_tokenizer\n\n    # Set models to evaluation mode\n    original_model.eval()\n    distilled_model.eval()\n    teacher_model.eval()\n\n    # Function to generate summary\n    def get_summary(model, tokenizer, text):\n        input_text = f\"summarize: {text}\"\n        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            start_time = time.time()\n            summary_ids = model.generate(\n                input_ids,\n                max_length=150,\n                num_beams=4,\n                early_stopping=True,\n                no_repeat_ngram_size=2\n            )\n            end_time = time.time()\n\n        inference_time = end_time - start_time\n        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n        return summary, inference_time\n\n    # Get summaries from all models\n    original_summary, original_time = get_summary(original_model, original_tokenizer, text)\n    distilled_summary, distilled_time = get_summary(distilled_model, distilled_tokenizer, text)\n    teacher_summary, teacher_time = get_summary(teacher_model, teacher_tokenizer, text)\n\n    # Calculate ROUGE score (if rouge is available)\n    try:\n        from rouge import Rouge\n        rouge = Rouge()\n\n        # Calculate ROUGE scores comparing to teacher model\n        original_scores = rouge.get_scores(original_summary, teacher_summary)[0]\n        distilled_scores = rouge.get_scores(distilled_summary, teacher_summary)[0]\n        rouge_available = True\n    except:\n        rouge_available = False\n        print(\"ROUGE scoring not available. Install it with: pip install rouge\")\n\n    # Print results\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPARISON OF MODELS\")\n    print(\"=\"*80)\n\n    print(\"\\nORIGINAL TEXT:\")\n    print(text[:500] + \"...\" if len(text) > 500 else text)\n\n    print(\"\\nTEACHER MODEL SUMMARY (T5-base):\")\n    print(f\"Time: {teacher_time:.4f} seconds\")\n    print(teacher_summary)\n\n    print(\"\\nORIGINAL STUDENT MODEL SUMMARY (T5-small):\")\n    print(f\"Time: {original_time:.4f} seconds\")\n    print(original_summary)\n\n    print(\"\\nDISTILLED STUDENT MODEL SUMMARY (T5-small distilled):\")\n    print(f\"Time: {distilled_time:.4f} seconds\")\n    print(distilled_summary)\n\n    # Print ROUGE scores if available\n    if rouge_available:\n        print(\"\\nROUGE SCORES (compared to teacher model):\")\n        print(f\"Original Student Model:\")\n        print(f\"  ROUGE-1: {original_scores['rouge-1']['f']:.4f}\")\n        print(f\"  ROUGE-2: {original_scores['rouge-2']['f']:.4f}\")\n        print(f\"  ROUGE-L: {original_scores['rouge-l']['f']:.4f}\")\n\n        print(f\"\\nDistilled Student Model:\")\n        print(f\"  ROUGE-1: {distilled_scores['rouge-1']['f']:.4f}\")\n        print(f\"  ROUGE-2: {distilled_scores['rouge-2']['f']:.4f}\")\n        print(f\"  ROUGE-L: {distilled_scores['rouge-l']['f']:.4f}\")\n\n        # Calculate improvement percentage\n        rouge1_improvement = ((distilled_scores['rouge-1']['f'] - original_scores['rouge-1']['f']) /\n                             original_scores['rouge-1']['f'] * 100)\n        rouge2_improvement = ((distilled_scores['rouge-2']['f'] - original_scores['rouge-2']['f']) /\n                             original_scores['rouge-2']['f'] * 100)\n        rougeL_improvement = ((distilled_scores['rouge-l']['f'] - original_scores['rouge-l']['f']) /\n                             original_scores['rouge-l']['f'] * 100)\n\n        print(f\"\\nImprovement from Distillation:\")\n        print(f\"  ROUGE-1: {rouge1_improvement:.2f}%\")\n        print(f\"  ROUGE-2: {rouge2_improvement:.2f}%\")\n        print(f\"  ROUGE-L: {rougeL_improvement:.2f}%\")\n\n    # Performance comparison\n    speed_improvement = ((original_time - distilled_time) / original_time) * 100\n    print(f\"\\nInference Speed Improvement: {speed_improvement:.2f}%\")\n\n    # Clean up to save memory\n    del original_model, distilled_model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Example usage:\n# First, import the time module if not already imported\nimport time\n\n# Sample text to test\nsample_text = \"\"\"\nClimate change is the long-term alteration of temperature and typical weather patterns in a place.\nClimate change could refer to a particular location or the planet as a whole. Climate change may\ncause weather patterns to be less predictable. These unexpected weather patterns can make it\ndifficult to maintain and grow crops in regions that rely on farming because expected temperature\nand rainfall levels can no longer be relied on. Climate change has also been connected with other\ndamaging weather events such as more frequent and more intense hurricanes, floods, downpours, and\nwinter storms. In polar regions, the warming global temperatures associated with climate change have\nmeant ice sheets and glaciers are melting at an accelerated rate from season to season.\n\"\"\"\n\n#compare_original_vs_distilled(sample_text)","metadata":{"id":"WcXn-rWP7Rvr","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:58:03.183361Z","iopub.execute_input":"2025-04-02T09:58:03.183687Z","iopub.status.idle":"2025-04-02T09:58:03.196330Z","shell.execute_reply.started":"2025-04-02T09:58:03.183647Z","shell.execute_reply":"2025-04-02T09:58:03.195627Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"compare_original_vs_distilled(sample_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WI36hTsn7TB-","outputId":"5d40a076-99f7-4f86-b823-c61a105433e0","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:58:11.107151Z","iopub.execute_input":"2025-04-02T09:58:11.107444Z","iopub.status.idle":"2025-04-02T09:58:16.819248Z","shell.execute_reply.started":"2025-04-02T09:58:11.107408Z","shell.execute_reply":"2025-04-02T09:58:16.818367Z"}},"outputs":[{"name":"stdout","text":"Loading original student model...\nLoading distilled model from t5_small_distilled_epoch_3...\n\n================================================================================\nCOMPARISON OF MODELS\n================================================================================\n\nORIGINAL TEXT:\n\nClimate change is the long-term alteration of temperature and typical weather patterns in a place.\nClimate change could refer to a particular location or the planet as a whole. Climate change may\ncause weather patterns to be less predictable. These unexpected weather patterns can make it\ndifficult to maintain and grow crops in regions that rely on farming because expected temperature\nand rainfall levels can no longer be relied on. Climate change has also been connected with other\ndamaging weath...\n\nTEACHER MODEL SUMMARY (T5-base):\nTime: 1.9858 seconds\nclimate change is the long-term alteration of temperature and typical weather patterns in a place . this can make it difficult to maintain and grow crops in regions that rely on farming if weather is less predictable, says nina dos santos, director of climate research at the u.s. ice sheets and glaciers are melting at an accelerated rate from season to season, she says.\n\nORIGINAL STUDENT MODEL SUMMARY (T5-small):\nTime: 1.2969 seconds\nclimate change is the long-term alteration of temperature and typical weather patterns in a place. it could refer to the particular location or the planet as an whole - climate changes may make it difficult to maintain and grow crops in regions that rely on farming because expected temperatures and rainfall levels can no longer be relied on. ice sheets and glaciers are melting at an accelerated rate from season to season.\n\nDISTILLED STUDENT MODEL SUMMARY (T5-small distilled):\nTime: 0.9070 seconds\nclimate change is the long-term alteration of temperature and typical weather patterns in a place. polar regions have been linked to more frequent and more intense hurricanes, flooding, downpours and winter storms.\n\nROUGE SCORES (compared to teacher model):\nOriginal Student Model:\n  ROUGE-1: 0.6964\n  ROUGE-2: 0.5821\n  ROUGE-L: 0.6786\n\nDistilled Student Model:\n  ROUGE-1: 0.4146\n  ROUGE-2: 0.2979\n  ROUGE-L: 0.3902\n\nImprovement from Distillation:\n  ROUGE-1: -40.46%\n  ROUGE-2: -48.83%\n  ROUGE-L: -42.49%\n\nInference Speed Improvement: 30.06%\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!pip install -q rouge","metadata":{"id":"DZBURMRE785L","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T11:33:37.162482Z","iopub.execute_input":"2025-03-26T11:33:37.162879Z","iopub.status.idle":"2025-03-26T11:33:40.713269Z","shell.execute_reply.started":"2025-03-26T11:33:37.162854Z","shell.execute_reply":"2025-03-26T11:33:40.712362Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import time\nimport torch\nimport gc\nfrom rouge import Rouge\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\ndef compare_student_vs_distilled(texts, original_model_path=\"t5-small\"):\n    \"\"\"\n    Compare the original student model with the distilled model using ROUGE scores.\n    High rouge indicates these two models' outputs are more similar, and it doesn't mean good quality.\n\n    Args:\n        texts (list): List of input texts to summarize\n        original_model_path (str): Path to the original model (default: \"t5-small\")\n    \"\"\"\n    # Initialize ROUGE\n    try:\n        rouge = Rouge()\n        print(\"ROUGE scoring initialized successfully\")\n    except Exception as e:\n        print(f\"Error initializing ROUGE: {e}\")\n        print(\"Installing ROUGE...\")\n        !pip install -q rouge\n        try:\n            from rouge import Rouge\n            rouge = Rouge()\n            print(\"ROUGE installed and initialized successfully\")\n        except Exception as e:\n            print(f\"Failed to install ROUGE: {e}\")\n            return\n\n    # Load the original student model\n    print(\"Loading original student model...\")\n    original_model = T5ForConditionalGeneration.from_pretrained(original_model_path).to(device)\n    original_tokenizer = T5Tokenizer.from_pretrained(original_model_path)\n\n    # Load the latest distilled model checkpoint\n    distilled_model_path = f\"t5_small_distilled_epoch_{num_epochs}\"\n\n    try:\n        print(f\"Loading distilled model from {distilled_model_path}...\")\n        distilled_model = T5ForConditionalGeneration.from_pretrained(distilled_model_path).to(device)\n        distilled_tokenizer = T5Tokenizer.from_pretrained(distilled_model_path)\n    except:\n        print(\"Couldn't find the distilled model checkpoint. Using current student model...\")\n        distilled_model = student_model\n        distilled_tokenizer = student_tokenizer\n\n    # Function to generate summary\n    def get_summary(model, tokenizer, text):\n        input_text = f\"summarize: {text}\"\n        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            start_time = time.time()\n            summary_ids = model.generate(\n                input_ids,\n                max_length=150,\n                num_beams=4,\n                early_stopping=True,\n                no_repeat_ngram_size=2\n            )\n            end_time = time.time()\n\n        inference_time = end_time - start_time\n        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n        return summary, inference_time\n\n    # Results storage\n    all_rouge1 = []\n    all_rouge2 = []\n    all_rougeL = []\n    all_orig_times = []\n    all_dist_times = []\n\n    # Process each text\n    for i, text in enumerate(texts):\n        print(f\"\\nProcessing text {i+1}/{len(texts)}\")\n\n        # Get summaries\n        original_summary, original_time = get_summary(original_model, original_tokenizer, text)\n        distilled_summary, distilled_time = get_summary(distilled_model, distilled_tokenizer, text)\n\n        # Track times\n        all_orig_times.append(original_time)\n        all_dist_times.append(distilled_time)\n\n        # Calculate ROUGE scores between original and distilled\n        scores = rouge.get_scores(distilled_summary, original_summary)[0]\n\n        # Store scores\n        all_rouge1.append(scores['rouge-1']['f'])\n        all_rouge2.append(scores['rouge-2']['f'])\n        all_rougeL.append(scores['rouge-l']['f'])\n\n        # Print individual results\n        print(f\"\\nText {i+1}:\")\n        print(f\"Original Summary ({original_time:.4f}s): {original_summary}\")\n        print(f\"Distilled Summary ({distilled_time:.4f}s): {distilled_summary}\")\n        print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n        print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n        print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n\n    # Calculate averages\n    avg_rouge1 = sum(all_rouge1) / len(all_rouge1)\n    avg_rouge2 = sum(all_rouge2) / len(all_rouge2)\n    avg_rougeL = sum(all_rougeL) / len(all_rougeL)\n    avg_orig_time = sum(all_orig_times) / len(all_orig_times)\n    avg_dist_time = sum(all_dist_times) / len(all_dist_times)\n\n    # Print summary report\n    print(\"\\n\" + \"=\"*80)\n    print(\"STUDENT VS DISTILLED MODEL COMPARISON\")\n    print(\"=\"*80)\n\n    print(\"\\nROUGE SCORES SUMMARY (higher means more similar):\")\n    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n\n    print(\"\\nSPEED COMPARISON:\")\n    speed_improvement = ((avg_orig_time - avg_dist_time) / avg_orig_time) * 100\n    print(f\"Original model average time: {avg_orig_time:.4f} seconds\")\n    print(f\"Distilled model average time: {avg_dist_time:.4f} seconds\")\n    print(f\"Speed improvement: {speed_improvement:.2f}%\")\n\n    # Interpretation\n    print(\"\\nINTERPRETATION:\")\n\n    if avg_rouge1 > 0.8 and avg_rouge2 > 0.6 and avg_rougeL > 0.7:\n        print(\"• The distilled model produces very similar summaries to the original student model\")\n    elif avg_rouge1 > 0.6 and avg_rouge2 > 0.4 and avg_rougeL > 0.5:\n        print(\"• The distilled model produces moderately similar summaries to the original student model\")\n    else:\n        print(\"• The distilled model produces somewhat different summaries from the original student model\")\n\n    if speed_improvement > 10:\n        print(f\"• The distilled model shows significant speed improvements ({speed_improvement:.1f}%)\")\n    elif speed_improvement > 0:\n        print(f\"• The distilled model shows modest speed improvements ({speed_improvement:.1f}%)\")\n    else:\n        print(f\"• The distilled model does not show speed improvements ({speed_improvement:.1f}%)\")\n\n    # Clean up memory\n    del original_model, distilled_model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Example texts for testing\ntest_texts = [\n    \"\"\"\n    Climate change is the long-term alteration of temperature and typical weather patterns in a place.\n    Climate change could refer to a particular location or the planet as a whole. Climate change may\n    cause weather patterns to be less predictable. These unexpected weather patterns can make it\n    difficult to maintain and grow crops in regions that rely on farming because expected temperature\n    and rainfall levels can no longer be relied on.\n    \"\"\",\n\n    \"\"\"\n    Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are\n    programmed to think like humans and mimic their actions. The term may also be applied to any machine\n    that exhibits traits associated with a human mind such as learning and problem-solving. The ideal\n    characteristic of artificial intelligence is its ability to rationalize and take actions that have\n    the best chance of achieving a specific goal.\n    \"\"\",\n\n    \"\"\"\n    The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of\n    coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2\n    (SARS-CoV-2). The novel virus was first identified from an outbreak in Wuhan, China, in December\n    2019. Attempts to contain it there failed, allowing it to spread across the globe.\n    \"\"\"\n]\n\n# compare_student_vs_distilled(test_texts)","metadata":{"id":"VOUkc2xt8ZYf","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:59:50.238867Z","iopub.execute_input":"2025-04-02T09:59:50.239206Z","iopub.status.idle":"2025-04-02T09:59:50.258377Z","shell.execute_reply.started":"2025-04-02T09:59:50.239176Z","shell.execute_reply":"2025-04-02T09:59:50.257595Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"compare_student_vs_distilled(test_texts)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZ9fkOtc8egU","outputId":"26fe9a6d-f300-4ef1-a186-c484ef5f3218","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T10:00:01.418531Z","iopub.execute_input":"2025-04-02T10:00:01.418857Z","iopub.status.idle":"2025-04-02T10:00:11.305443Z","shell.execute_reply.started":"2025-04-02T10:00:01.418827Z","shell.execute_reply":"2025-04-02T10:00:11.304681Z"}},"outputs":[{"name":"stdout","text":"Error initializing ROUGE: local variable 'Rouge' referenced before assignment\nInstalling ROUGE...\nROUGE installed and initialized successfully\nLoading original student model...\nLoading distilled model from t5_small_distilled_epoch_3...\n\nProcessing text 1/3\n\nText 1:\nOriginal Summary (1.0589s): climate change is the long-term alteration of temperature and typical weather patterns in a place. climate changes could refer to particular location or the planet as an whole - and may cause weather pattern to be less predictable. if the weather is not predictable, it can make it difficult to maintain and grow crops in regions that rely on farming because expected temperatures and rainfall levels can no longer be relied on.\nDistilled Summary (0.7876s): climate change is the long-term alteration of temperature and typical weather patterns in a place. this could refer to particular location or the planet asa whole. Climate change can make it difficult to maintain and grow crops in regions that rely on farming because expected temperatures and rainfall levels cannot be relied on.\nROUGE-1: 0.8235\nROUGE-2: 0.6992\nROUGE-L: 0.8235\n\nProcessing text 2/3\n\nText 2:\nOriginal Summary (0.9716s): artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. the term may also be applied to any machine that exhibit traits associated with a human mind such as learning and problem-solving.\nDistilled Summary (0.9818s): artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. the term may also be applied to any machine that exhibit traits associated with a human mind such as learning and problem-solving.\nROUGE-1: 1.0000\nROUGE-2: 1.0000\nROUGE-L: 1.0000\n\nProcessing text 3/3\n\nText 3:\nOriginal Summary (0.5061s): the novel virus was first identified from an outbreak in Wuhan, china, in December 2019. Attempts to contain it failed, allowing it to spread across the globe.\nDistilled Summary (0.4744s): the novel virus was first identified from an outbreak in Wuhan, China, in December 2019. Attempts to contain it failed, allowing it to spread across the globe.\nROUGE-1: 0.9565\nROUGE-2: 0.9231\nROUGE-L: 0.9565\n\n================================================================================\nSTUDENT VS DISTILLED MODEL COMPARISON\n================================================================================\n\nROUGE SCORES SUMMARY (higher means more similar):\nAverage ROUGE-1: 0.9267\nAverage ROUGE-2: 0.8741\nAverage ROUGE-L: 0.9267\n\nSPEED COMPARISON:\nOriginal model average time: 0.8455 seconds\nDistilled model average time: 0.7479 seconds\nSpeed improvement: 11.55%\n\nINTERPRETATION:\n• The distilled model produces very similar summaries to the original student model\n• The distilled model shows significant speed improvements (11.5%)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import time\n# Function to compare rouge metric of different models' summary against reference summary\ndef compare_models_with_references(test_texts, test_references, original_model_path=\"t5-small\"):\n    \"\"\"\n    Compare original and distilled models using reference summaries from the dataset\n    \n    Args:\n        test_texts (list): List of input texts from test set\n        test_references (list): Corresponding reference summaries\n        original_model_path (str): Path to original model\n    \"\"\"\n    # Initialize ROUGE\n    try:\n        rouge = Rouge()\n    except:\n        !pip install -q rouge\n        from rouge import Rouge\n        rouge = Rouge()\n\n    # Load models \n    print(\"Loading tacher student model...\")\n    teacher_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n    teacher_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    print(\"Loading original student model...\")\n    original_model = T5ForConditionalGeneration.from_pretrained(original_model_path).to(device)\n    original_tokenizer = T5Tokenizer.from_pretrained(original_model_path)\n\n    print(\"Loading distilled model...\")\n    try:\n        distilled_model = T5ForConditionalGeneration.from_pretrained(f\"t5_small_distilled_epoch_{num_epochs}\").to(device)\n        distilled_tokenizer = T5Tokenizer.from_pretrained(f\"t5_small_distilled_epoch_{num_epochs}\")\n    except:\n        distilled_model = student_model\n        distilled_tokenizer = student_tokenizer\n\n    # Modified generation function to include reference\n    def evaluate_model(model, tokenizer, texts, references):\n        model.eval()\n        generated_summaries = []\n        inference_times = []\n        \n        for text in texts:\n            input_text = f\"summarize: {text}\"\n            input_ids = tokenizer.encode(input_text,max_length=512,         \n                truncation=True, return_tensors=\"pt\").to(device)\n            \n            start_time = time.time()\n            with torch.no_grad():\n                summary_ids = model.generate(\n                    input_ids,\n                    max_length=150,\n                    num_beams=4,\n                    early_stopping=True\n                )\n            inference_time = time.time() - start_time\n            \n            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n            generated_summaries.append(summary)\n            inference_times.append(inference_time)\n            \n        # Calculate ROUGE scores against references\n        scores = rouge.get_scores(generated_summaries, references, avg=True)\n        return scores, np.mean(inference_times)\n\n    # Evaluate models\n    print(\"\\nEvaluating Teacher Model...\")\n    teacher_scores, teacher_avg_time = evaluate_model(teacher_model, teacher_tokenizer, test_texts, test_references)\n    \n    print(\"\\nEvaluating Original Model...\")\n    orig_scores, orig_avg_time = evaluate_model(original_model, original_tokenizer, test_texts, test_references)\n    \n    print(\"\\nEvaluating Distilled Model...\")\n    dist_scores, dist_avg_time = evaluate_model(distilled_model, distilled_tokenizer, test_texts, test_references)\n\n    # Print comparison report\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODEL COMPARISON AGAINST REFERENCE SUMMARIES\")\n    print(\"=\"*80)\n    \n    print(f\"\\n{'Metric':<10} | {'Teacher Model':<15}|{'Original Model':<15} | {'Distilled Model':<15} | Improvement\")\n    print(\"-\"*65)\n    \n    for metric in ['rouge-1', 'rouge-2', 'rouge-l']:\n        teacher_f=teacher_scores[metric]['f']\n        orig_f = orig_scores[metric]['f']\n        dist_f = dist_scores[metric]['f']\n        improvement = dist_f - orig_f\n        print(f\"{metric.upper():<10} | {teacher_f:.4f}{'':<5}|{orig_f:.4f}{'':<5} | {dist_f:.4f}{'':<5} | {improvement:+.4f}\")\n\n    print(\"\\nSpeed Comparison:\")\n    print(f\"Original Model Average Inference Time: {orig_avg_time:.4f}s\")\n    print(f\"Distilled Model Average Inference Time: {dist_avg_time:.4f}s\")\n    print(f\"Speed Improvement: {(orig_avg_time - dist_avg_time)/orig_avg_time*100:.2f}%\")\n\n    # Clean up\n    del original_model, distilled_model\n    torch.cuda.empty_cache()\n\n# Load CNN/DailyMail test data\ncnn_test = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\ntest_texts = cnn_test[\"article\"][:10]\ntest_references = cnn_test[\"highlights\"][:10]\n\n# Run comparison\ncompare_models_with_references(test_texts, test_references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T10:42:46.880992Z","iopub.execute_input":"2025-04-02T10:42:46.881335Z","iopub.status.idle":"2025-04-02T10:43:19.575784Z","shell.execute_reply.started":"2025-04-02T10:42:46.881305Z","shell.execute_reply":"2025-04-02T10:43:19.574873Z"}},"outputs":[{"name":"stdout","text":"Loading tacher student model...\nLoading original student model...\nLoading distilled model...\n\nEvaluating Teacher Model...\n\nEvaluating Original Model...\n\nEvaluating Distilled Model...\n\n================================================================================\nMODEL COMPARISON AGAINST REFERENCE SUMMARIES\n================================================================================\n\nMetric     | Teacher Model  |Original Model  | Distilled Model | Improvement\n-----------------------------------------------------------------\nROUGE-1    | 0.2723     |0.3205      | 0.2944      | -0.0261\nROUGE-2    | 0.0996     |0.1230      | 0.1220      | -0.0011\nROUGE-L    | 0.2495     |0.2911      | 0.2858      | -0.0053\n\nSpeed Comparison:\nOriginal Model Average Inference Time: 0.7176s\nDistilled Model Average Inference Time: 0.7134s\nSpeed Improvement: 0.58%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## Build User interface","metadata":{}},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T10:04:48.008382Z","iopub.execute_input":"2025-04-02T10:04:48.008715Z","iopub.status.idle":"2025-04-02T10:04:57.899917Z","shell.execute_reply.started":"2025-04-02T10:04:48.008686Z","shell.execute_reply":"2025-04-02T10:04:57.899158Z"}},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.8.0 (from gradio)\n  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a2)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio) (2024.12.0)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.29.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\nSuccessfully installed fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import gradio as gr\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nimport time\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load original student model\ndef load_original_model():\n    print(\"Loading original T5-small model...\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n    return model, tokenizer\n\n# Load distilled model from Hugging Face\ndef load_distilled_model(distilled_path=\"Wenfi/distillation-T5-cnn\"):\n    print(f\"Loading distilled model from Hugging Face: {distilled_path}...\")\n    try:\n        model = T5ForConditionalGeneration.from_pretrained(distilled_path).to(device)\n        tokenizer = T5Tokenizer.from_pretrained(distilled_path)\n    except Exception as e:\n        print(f\"Error loading distilled model: {e}\")\n        print(\"Using the original model path as fallback...\")\n        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n    return model, tokenizer\n\n# Load both models at startup\noriginal_model, original_tokenizer = load_original_model()\ndistilled_model, distilled_tokenizer = load_distilled_model()\n\n# Generate summary function\ndef generate_summary(model, tokenizer, text, max_length=150):\n    # Prepare input\n    input_text = f\"summarize: {text}\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # Generate summary and measure time\n    start_time = time.time()\n    summary_ids = model.generate(\n        input_ids,\n        max_length=max_length,\n        num_beams=4,\n        early_stopping=True,\n        no_repeat_ngram_size=2\n    )\n    end_time = time.time()\n    inference_time = end_time - start_time\n\n    # Decode summary\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary, inference_time\n\n# Function for original model button\ndef summarize_with_original(text):\n    if not text.strip():\n        return \"Please enter some text to summarize.\"\n\n    summary, inference_time = generate_summary(original_model, original_tokenizer, text)\n    return f\"Original T5-small Summary (took {inference_time:.4f} seconds):\\n\\n{summary}\"\n\n# Function for distilled model button\ndef summarize_with_distilled(text):\n    if not text.strip():\n        return \"Please enter some text to summarize.\"\n\n    summary, inference_time = generate_summary(distilled_model, distilled_tokenizer, text)\n    return f\"Distilled T5-small Summary (took {inference_time:.4f} seconds):\\n\\n{summary}\"\n\n# Create Gradio interface\ndef create_ui():\n    with gr.Blocks(title=\"T5 Summarization Models Comparison\") as demo:\n        gr.Markdown(\"# Compare Original vs Distilled T5 Summarization Models\")\n        gr.Markdown(\"Enter text below and click on either button to generate a summary using that model.\")\n\n        with gr.Row():\n            text_input = gr.Textbox(\n                lines=10,\n                placeholder=\"Enter text to summarize here...\",\n                label=\"Input Text\"\n            )\n\n        with gr.Row():\n            original_button = gr.Button(\"Summarize with Original T5-small\", variant=\"primary\")\n            distilled_button = gr.Button(\"Summarize with Distilled T5-small\", variant=\"primary\")\n\n        with gr.Row():\n            output = gr.Textbox(lines=8, label=\"Summary Output\")\n\n        original_button.click(\n            fn=summarize_with_original,\n            inputs=text_input,\n            outputs=output\n        )\n\n        distilled_button.click(\n            fn=summarize_with_distilled,\n            inputs=text_input,\n            outputs=output\n        )\n\n        gr.Markdown(\"\"\"\n        ## About the Models\n        - **Original T5-small**: The baseline T5-small model (60M parameters)\n        - **Distilled T5-small**: A T5-small model distilled from T5-base (220M parameters)\n           from HuggingFace: [ooor/t5-small-distilled-summarization](https://huggingface.co/Wenfi/distillation-T5-cnn)\n\n        The distilled model should provide similar quality summaries but potentially with faster inference.\n        And we are ChunkyPanda:)\n        from team12\n        \"\"\")\n\n    return demo\n\n# Launch the UI\nif __name__ == \"__main__\":\n    # Install Gradio if not already installed\n    # !pip install -q gradio\n\n    # Create and launch the UI\n    demo = create_ui()\n    demo.launch(share=True)  # Set share=False if you don't want a public link","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T10:08:03.486260Z","iopub.execute_input":"2025-04-02T10:08:03.486557Z","iopub.status.idle":"2025-04-02T10:08:15.347572Z","shell.execute_reply.started":"2025-04-02T10:08:03.486531Z","shell.execute_reply":"2025-04-02T10:08:15.346711Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading original T5-small model...\nLoading distilled model from Hugging Face: Wenfi/distillation-T5-cnn...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0cf44e004347bcaf05ecce65e0e30b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9263d5ba8b34459b303b49949d5238a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ca3b53782a4c91abc29ff2640d5974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e09f698209466f9b11c8517d73cec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63c5a110e2194bef9f4c6fed37268d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e9cafebac04fc1b7bc446d40aead8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd74d0d74fd64a0c8f49f568932e2897"}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://ab869e13af3d4c35aa.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ab869e13af3d4c35aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":33}]}